# Results & Analysis ğŸ“Š

## **Union of Intervals - Problem Description**

In this assignment, we study the hypothesis class of a **finite union of disjoint intervals** and analyze the properties of the **Empirical Risk Minimization (ERM) algorithm** for this class.

Let the sample space be ğ’³ = [ğŸ¢,ğŸ£] and consider a **binary classification problem**, where ğ’´ = \{ğŸ¢,ğŸ£\} . We aim to learn using a hypothesis class consisting of **ğ’Œ disjoint intervals**.

### **Mathematical Formulation**
We define a hypothesis class of disjoint intervals as follows:

Let ğ¼ be a set of  ğ‘˜ disjoint intervals:
ğ¼ = \{[ğ“â‚, ğ“Šâ‚], [ğ“â‚‚, ğ“Šâ‚‚], ..., [ğ“â‚–, ğ“Šâ‚–]\}
where the intervals are ordered such that:

ğŸ¢ â‰¤ ğ“â‚ â‰¤ ğ“Šâ‚ â‰¤ ğ“â‚‚ â‰¤ ğ“Šâ‚‚ â‰¤ ... â‰¤ ğ“Šâ‚– â‰¤ ğŸ£

For each such set ğ¼, define the corresponding hypothesis function:
<img width="775" alt="Screenshot 2025-02-11 at 15 04 49" src="https://github.com/user-attachments/assets/38b17e99-462b-470d-beb7-5e0ccae55196" />

The hypothesis class â„‹â‚– consists of all possible such hypotheses:
â„‹â‚– = \{ â„â‚— | I = \{[ğ“â‚, ğ“Šâ‚], ..., [ğ“â‚–, ğ“Šâ‚–]\}, ğŸ¢ â‰¤ ğ“â‚ â‰¤ ğ“Šâ‚ â‰¤ ğ“â‚‚ â‰¤ ğ“Šâ‚‚ â‰¤ ... â‰¤ ğ“Šâ‚– â‰¤ ğŸ£ \}

We are given a labeled sample of size n:
 (ğ‘¥â‚, ğ‘¦â‚), ..., (ğ‘¥â‚™, ğ‘¦â‚™)
where the points xáµ¢ are sorted in increasing order:  ğŸ¢ â‰¤ ğ‘¥â‚ < ğ‘¥â‚‚ < ... < ğ‘¥â‚™ â‰¤ ğŸ£.

---

## **(a) Hypothesis Selection with Minimum Error**

### **Problem Statement**
We assume the true distribution  ğ‘ƒ[ğ‘¥,ğ‘¦] = ğ‘ƒ[ğ‘¦|ğ‘¥]âˆ™ğ‘ƒ[ğ‘¥] is given as follows:
- ğ‘¥ is uniformly distributed over [ğŸ¢,ğŸ£].
- The conditional probability ğ‘ƒ\[ğ‘¦ = ğŸ£|ğ‘¥\] is defined as:
<img width="741" alt="Screenshot 2025-02-11 at 15 15 29" src="https://github.com/user-attachments/assets/bf4999d3-b2c9-424e-857a-9e852752e064" />

- Since ğ‘ƒ\[ğ‘¦ = ğŸ¢|ğ‘¥\] = ğŸ£ - ğ‘ƒ\[ğ‘¦ = ğŸ£|ğ‘¥\], we can compute the exact error ğ‘’â‚š(â„) for any hypothesis â„âˆŠâ„‹â‚–.

### **Solution**
For â„ âˆŠ â„‹â‚â‚€, the error is computed as:
<img width="576" alt="Screenshot 2025-02-11 at 16 07 23" src="https://github.com/user-attachments/assets/e99bebb4-abae-4fcd-9b0b-4e4e4ef5b24d" />

Since ğ‘‹ is uniformly distributed over [ğŸ¢,ğŸ£], we use ğ‘ƒ\(ğ‘‹,ğ‘Œ\) = ğ‘ƒ\(ğ‘Œ|ğ‘‹\)ğ‘ƒ\(ğ‘‹\) to rewrite:

<img width="619" alt="Screenshot 2025-02-11 at 16 08 00" src="https://github.com/user-attachments/assets/c877edb5-a76d-4381-8a2d-724870648204" />

We focus only on cases where Î”ğ‘§â‚’ â‰  0 , meaning incorrect predictions:
- Iâ‚: Intervals where â„(ğ‘‹)=ğŸ£ and ğ‘ƒ\(ğ‘Œ=ğŸ£|ğ‘‹\) = 0.8 (no error, ignored)
- Iâ‚‚: Intervals where â„(ğ‘‹)=ğŸ£ and ğ‘ƒ\(ğ‘Œ=ğŸ£|ğ‘‹\) = 0.1 (error occurs)
- Iâ‚ƒ: Intervals where â„(ğ‘‹)=ğŸ¢ and ğ‘ƒ\(ğ‘Œ=ğŸ£|ğ‘‹\) = 0.8 (error occurs)
- Iâ‚„: Intervals where â„(ğ‘‹)=ğŸ¢  and ğ‘ƒ\(ğ‘Œ=ğŸ£|ğ‘‹\) = 0.1 (no error, ignored)

Thus, the expected error simplifies to:
<img width="460" alt="Screenshot 2025-02-11 at 16 08 22" src="https://github.com/user-attachments/assets/137f286c-7ef5-4630-801e-1a3b1f27fa0f" />

To minimize error, we aim to keep Iâ‚‚ and Iâ‚ƒ as small as possible. One approach is to introduce a small unit ğœ€ > 0 at the interval edges, ensuring exactly 10 disjoint segments:
<img width="588" alt="Screenshot 2025-02-11 at 16 08 43" src="https://github.com/user-attachments/assets/e9454114-e5eb-426f-822c-ef916e588b8a" />

where:
- ğ‘›â‚ is the number of intervals added to the complement of [0,0.2] âˆª [0.4,0.6] âˆª [0.8,1]
- ğ‘›â‚‚ is the number of intervals removed from [0,0.2] âˆª [0.4,0.6] âˆª [0.8,1]

This ensures the best hypothesis  â„ âˆŠ â„‹â‚â‚€ has the smallest possible classification error.

---

## **(b) Empirical and True Error Analysis**

### **Problem Statement**
We implement a function that calculates the true error ğ‘’â‚š(â„â‚—) for a given list of intervals ğ¼ . Then, for ğ‘˜ = 3  and various values of ğ‘› (10, 15, 20, ..., 100), we conduct the following experiment ğ‘‡ = 100 times:
1. Draw a sample of size ğ‘› and run the **ERM algorithm**.
2. Calculate the **empirical error** for the returned hypothesis.
3. Calculate the **true error** for the returned hypothesis.
4. Plot the empirical and true errors, averaged across the ğ‘‡ runs, as a function of ğ‘›.
5. Discuss the results: Do the empirical and true errors decrease or increase with ğ‘›? Why?

### **Solution & Analysis**
- The **empirical error** increases as ğ‘› grows, but its **rate of increase slows down**, which aligns with theoretical expectations. A **larger sample size provides more information about the true distribution**, allowing for a better hypothesis selection.
- The **true error remains relatively constant**, with slight fluctuations. This behavior is expected because the true error is an inherent property of the hypothesis and the true distribution ğ‘ƒ, which do not change with sample size.
- Overall, we observe the expected trend where **as ğ’ increases, the empirical and true errors converge**, demonstrating the consistency of the ERM algorithm.
#### **Graph:**
![Empirial VS true Error](empiricalVStrueError.png)

---

## **(c) Error Behavior as a Function of ğ’Œ**

### **Problem Statement**
We draw a sample of size ğ‘› = 1500 and find the best ERM hypothesis for ğ‘˜ = 1, 2, ..., 10. We then plot the empirical and true errors as a function of ğ‘˜ and analyze how the error behaves. We also define ğ‘˜* to be the ğ‘˜ with the smallest empirical error for ERM and discuss whether this makes ğ‘˜* a good choice.

### **Solution & Analysis**
- In this experiment, we observed that ğ‘˜* = 9  yielded the smallest empirical error.
- However, this does not necessarily mean that <img width="36" alt="Screenshot 2025-02-11 at 16 09 26" src="https://github.com/user-attachments/assets/a666b2a8-1748-4854-8a3c-c37e6326464b" /> is the best hypothesis class. This is because it may lead to **overfitting**, where the selected hypothesis is overly complex and fits the training set too closely, capturing unnecessary noise.
- A hypothesis that generalizes well should have both **low true error** and **low empirical error**. While ğ‘˜* minimizes the empirical error, it may not perform well on new data.
- A simpler model with a slightly higher empirical error but a lower true error could be a better choice for generalization.

#### **Graph:**
![Empirial and true Error](empiricalANDtrueError.png)

